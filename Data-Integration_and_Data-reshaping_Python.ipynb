{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<a id=\"introduction\"></a>\n",
    "\n",
    "Here are two main targets in this assignment. \n",
    "\n",
    "First is to integrate several datasets into one single schema and input some data and fix possible problems in this final data.\n",
    "\n",
    "Second is to find which attributes( Rooms, crime_C_average,travel_min_to_CBD, and property_age) is important to predict the price when we want to build a linear model. In addition, in this part we will use different normalization/ transformation metods, such as standardization, min-max normalization, log, power, and root transformation on attributes to observe and explore the effect on the linear model we want to build.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "<a id=\"importlibraries\"></a>\n",
    "Import some libraries for this assignment:\n",
    "* pandas\n",
    "* re \n",
    "* numpy \n",
    "* sklearn\n",
    "* BeautifulSoup\n",
    "* geopandas\n",
    "* pygtfs\n",
    "* shapely\n",
    "* math\n",
    "* datetime\n",
    "* urllib\n",
    "* difflib\n",
    "* zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.12/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: bs4 in ./venv/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv/lib/python3.12/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: geopandas in ./venv/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.22 in ./venv/lib/python3.12/site-packages (from geopandas) (2.1.2)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in ./venv/lib/python3.12/site-packages (from geopandas) (0.10.0)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from geopandas) (24.1)\n",
      "Requirement already satisfied: pandas>=1.4.0 in ./venv/lib/python3.12/site-packages (from geopandas) (2.2.3)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in ./venv/lib/python3.12/site-packages (from geopandas) (3.7.0)\n",
      "Requirement already satisfied: shapely>=2.0.0 in ./venv/lib/python3.12/site-packages (from geopandas) (2.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas>=1.4.0->geopandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas>=1.4.0->geopandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas>=1.4.0->geopandas) (2024.2)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from pyogrio>=0.7.2->geopandas) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pygtfs in ./venv/lib/python3.12/site-packages (0.1.9)\n",
      "Requirement already satisfied: sqlalchemy>=0.7.8 in ./venv/lib/python3.12/site-packages (from pygtfs) (2.0.35)\n",
      "Requirement already satisfied: pytz>=2014.9 in ./venv/lib/python3.12/site-packages (from pygtfs) (2024.2)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.12/site-packages (from pygtfs) (1.16.0)\n",
      "Requirement already satisfied: docopt in ./venv/lib/python3.12/site-packages (from pygtfs) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in ./venv/lib/python3.12/site-packages (from sqlalchemy>=0.7.8->pygtfs) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./venv/lib/python3.12/site-packages (from sqlalchemy>=0.7.8->pygtfs) (3.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install bs4\n",
    "!pip install geopandas\n",
    "!pip install pygtfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talbe of contents\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Import libraries](#importlibraries)\n",
    "* [Task 1: Data Integration](#task_1)\n",
    "    * [Step 01: Find the suburb](#step01)\n",
    "    * [Step 02: Find the closest train station id and distance(m)](#step02)\n",
    "    * [Step 03: Calculate the average travel time from the closest train station to Southern Cross Station](#step03)\n",
    "    * [Step 04: over priced?](#step04)\n",
    "    * [Step 05: crime A/B/C average](#step05)\n",
    "    * [Step 06: Find the closest primary/ Secondary school and the distance(m) between property](#step06)\n",
    "    * [Step 07: Primary / Secondary School Ranking](#step07)\n",
    "\n",
    "* [Taks 2: Data reshaping](#task_2)\n",
    "\n",
    "* [Summary](#summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/DataspellProjects/Data-Integration_and_Data-reshaping_Python/venv/lib/python3.12/site-packages/pandas/compat/_optional.py:135\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1324\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m crimebylocationdatatable_yearending31march2016 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcrimebylocationdatatable-yearending31march2016.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m crimebylocationdatatable_yearending31march2016\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m crimebylocationdatatable_yearending31march2016\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m crimebylocationdatatable_yearending31march2016 \u001b[38;5;241m=\u001b[39m crimebylocationdatatable_yearending31march2016[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/DataspellProjects/Data-Integration_and_Data-reshaping_Python/venv/lib/python3.12/site-packages/pandas/io/excel/_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n",
      "File \u001b[0;32m~/DataspellProjects/Data-Integration_and_Data-reshaping_Python/venv/lib/python3.12/site-packages/pandas/io/excel/_base.py:1567\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m engine\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options \u001b[38;5;241m=\u001b[39m storage_options\n\u001b[0;32m-> 1567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_io\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DataspellProjects/Data-Integration_and_Data-reshaping_Python/venv/lib/python3.12/site-packages/pandas/io/excel/_openpyxl.py:552\u001b[0m, in \u001b[0;36mOpenpyxlReader.__init__\u001b[0;34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m     engine_kwargs: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    540\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    541\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03m    Reader using openpyxl engine.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;124;03m        Arbitrary keyword arguments passed to excel engine.\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 552\u001b[0m     \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenpyxl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    554\u001b[0m         filepath_or_buffer,\n\u001b[1;32m    555\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    556\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[1;32m    557\u001b[0m     )\n",
      "File \u001b[0;32m~/DataspellProjects/Data-Integration_and_Data-reshaping_Python/venv/lib/python3.12/site-packages/pandas/compat/_optional.py:138\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "crimebylocationdatatable_yearending31march2016 = pd.read_excel('crimebylocationdatatable-yearending31march2016.xlsx', index_col=0)\n",
    "crimebylocationdatatable_yearending31march2016.columns = crimebylocationdatatable_yearending31march2016.iloc[0]\n",
    "crimebylocationdatatable_yearending31march2016 = crimebylocationdatatable_yearending31march2016[1:]\n",
    "crimebylocationdatatable_yearending31march2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pygtfs\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape, Point\n",
    "import fiona\n",
    "from math import *\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import difflib\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import sys \n",
    "from sklearn import preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.formula.api as sm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Data Integration\n",
    "<a id=\"task_1\"></a>\n",
    "In this task, integrate some datasets into on with the following schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the csv file\n",
    "df = pd.read_csv(\"Group081.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the suburb\n",
    "<a id=\"step01\"></a>\n",
    "\n",
    "The target in this step is to find the property in which suburb.\n",
    "\n",
    "We use the Vic_suburb_boundary.zip file to set the suburb boundary, and if the property's lattitude and longtitude are included in the suburb's lattitude and longtitude, we fill the suburb for this property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the zip file of vic_suburb_boundary.zip\n",
    "zip_boundary = ZipFile('vic_suburb_boundary.zip')\n",
    "zip_boundary.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the VIC_LOCALITY_POLYGON_shp.shp by using fiona libary\n",
    "collection = fiona.open('VIC_LOCALITY_POLYGON_shp.shp')\n",
    "\n",
    "shapes = {}\n",
    "townnames = {}\n",
    "for f in collection:\n",
    "    # the surburb id\n",
    "    town_id = f['properties']['LC_PLY_PID']  \n",
    "    # the lattitude and longtitude boundary of surburb\n",
    "    shapes[town_id] = shape(f['geometry']) \n",
    "    # the surburb name\n",
    "    townnames[town_id] =f['properties']['VIC_LOCA_2']\n",
    "    \n",
    "def search(lat, long):  \n",
    "    global shapes, townnames\n",
    "    #if the lattitude and longtitude are included in the suburb's lattitude and longtitude, return suburb name.\n",
    "    return next((townnames[town_id]  for town_id in shapes if shapes[town_id].contains(Point(lat, long))), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the search hunction to the df\n",
    "df['Suburb']=df.apply(lambda x: search(x.Longtitude, x.Lattitude),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the train station id and station distance (m)\n",
    "<a id=\"step02\"></a>\n",
    "The target in this part is to find the closest train staion to the property which has a direct trip to the Southern Cross Railway Station and the direct distance from the closest train station to the property.\n",
    "\n",
    "In this section, we first extract gtfs file to find the stations and trips data, and then create get distance function to calculate the distance from the property to the cloest train station.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the gtfs file\n",
    "zip_gtfs = ZipFile('gtfs.zip')\n",
    "zip_gtfs.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a schedule object (a sqlite database)\n",
    "sched = pygtfs.Schedule(\":memory:\")                \n",
    "# append data to schedule object\n",
    "pygtfs.append_feed(sched, \"./1/google_transit.zip\")\n",
    "pygtfs.append_feed(sched, \"./2/google_transit.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this step, we find the station name and station's Longtitude and Lattitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the query of trips in schedule\n",
    "sched_trips=sched.trips_query\n",
    "# get the query of services in schedule\n",
    "sched_services=sched.services_query\n",
    "# get the query of stops in schedule\n",
    "sched_stops=sched.stops_query\n",
    "# get the query of stop_times in schedule\n",
    "sched_stop_time=sched.stop_times_query.all()\n",
    "\n",
    "\n",
    "\n",
    "#stops={}\n",
    "stops_id=[]\n",
    "station_name=[]\n",
    "station_lat_lon=[]\n",
    "\n",
    "# for loop in the query of stops\n",
    "for i in sched_stops.all():\n",
    "    stop_id=i.stop_id\n",
    "    stop_name=i.stop_name\n",
    "    stop_lat=i.stop_lat\n",
    "    stop_lon=i.stop_lon\n",
    "    #stops[stop_id]={'stop_name':stop_name,'stop_lat':stop_lat,'stop_lon':stop_lon}\n",
    "    lat_lon=[stop_lat,stop_lon]\n",
    "    #append stop name to station_name list\n",
    "    \n",
    "    if stop_id not in stops_id:\n",
    "        station_name.append(stop_name)\n",
    "        #append lat and long to lat_lon list\n",
    "        station_lat_lon.append(lat_lon)\n",
    "        #append stop id to stop_id list\n",
    "        stops_id.append(stop_id)\n",
    "\n",
    "# translate the list of station_lat_lon to matrix\n",
    "station_lat_lon_mat=np.matrix(station_lat_lon)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this step, we find the service ids that are work in weekday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the query of services in schedule\n",
    "sched_service=sched.services_query.all()\n",
    "weekday_service_id=[]\n",
    "\n",
    "#for loop in sched_service\n",
    "for i in sched_service:\n",
    "    service_id=i.service_id\n",
    "    M=int(i.monday)\n",
    "    T=int(i.tuesday)\n",
    "    W=int(i.wednesday)\n",
    "    Th=int(i.thursday)\n",
    "    F=int(i.friday)\n",
    "    \n",
    "    #if M+T+W+Th+F >=1, append service_id to weekday_service_id\n",
    "    if M+T+W+Th+F == 5 and service_id not in weekday_service_id:\n",
    "        weekday_service_id.append(service_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this step, we filter trips that are work in weekday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_trip=[]\n",
    "#for loop in the query of services in schedule\n",
    "for i in sched_trips.all():\n",
    "    route_id=i.route_id\n",
    "    service_id=i.service_id\n",
    "    trip_id=i.trip_id\n",
    "    #if service_id in weekday_service_id, append trip_id to weekday_trip\n",
    "    if service_id in weekday_service_id:\n",
    "        weekday_trip.append(trip_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this step, we find trips that are work in weekday and departrue time between 8:00 am to 9:30am and combine with the station id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "southern_cross_stop_id=['20043','22180']\n",
    "# set a dictionary to store the trip and stop station id\n",
    "trip_stop={}\n",
    "\n",
    "# for loop in the query of stop_time\n",
    "for i in sched_stop_time:\n",
    "    stop_list=[]\n",
    "    stop_id=i.stop_id\n",
    "    trip_id=i.trip_id\n",
    "    # append stop_id to stop_list\n",
    "    stop_list.append(stop_id)\n",
    "    arrival_time=i.arrival_time\n",
    "    departure_time=i.departure_time\n",
    "    # if departure_time between '7:00:00' and '9:30:00', trip_id in trip_list, stop_id equals to station_id, append departure_time to departure_time_list\n",
    "    if str(departure_time) >= '7:00:00' and str(departure_time) < '9:30:00' and trip_id in weekday_trip: \n",
    "        if trip_id not in trip_stop.keys():\n",
    "            trip_stop[trip_id]=stop_list\n",
    "        else:\n",
    "            original_value=trip_stop[trip_id]\n",
    "            original_value.append(stop_id)\n",
    "            trip_stop[trip_id]=original_value\n",
    "    # if trip_id in trip_stop.keys and if stop_id equals to southern_corss_stop_id, add the value to trip_stop[trip_id]\n",
    "    elif trip_id in trip_stop.keys():\n",
    "        if stop_id == southern_cross_stop_id[0]:\n",
    "            original_value=trip_stop[trip_id]\n",
    "            original_value.append(southern_cross_stop_id[0])\n",
    "            trip_stop[trip_id]=original_value\n",
    "        elif stop_id == southern_cross_stop_id[1]:\n",
    "            original_value=trip_stop[trip_id]\n",
    "            original_value.append(southern_cross_stop_id[1])\n",
    "            trip_stop[trip_id]=original_value\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the get_distance function that calculate the distance between property and station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(target,house):\n",
    "    # radius of equator: KM\n",
    "    ra = 6378  \n",
    "    \n",
    "    # change angle to radians\n",
    "    target_rad_lat = np.radians(target[:,0])\n",
    "    target_rad_lon = np.radians(target[:,1])\n",
    "    house_rad_lat = np.radians(house[:,0])\n",
    "    house_rad_lon = np.radians(house[:,1])\n",
    "    \n",
    "    dlon = house_rad_lon - target_rad_lon\n",
    "    dlat = house_rad_lat - target_rad_lat\n",
    "    a=np.power((np.sin(dlat/2)),2)+ np.multiply(np.multiply(np.cos(target_rad_lat),np.cos(house_rad_lat)),np.power((np.sin(dlon/2.0)),2))\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    distance = ra * c *1000\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the find_station function to find the closest station and the distance between this station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_station_name(lat,lon):\n",
    "    Find_loc = np.matrix([[lat,lon]])\n",
    "    #run get_distance function to get all distance between station.\n",
    "    all_distance = get_distance(station_lat_lon_mat,Find_loc)\n",
    "    #list 100 lowest distance\n",
    "    rank_100=list(all_distance.argsort(axis=0)[:100])\n",
    "    #for loop in rank_100\n",
    "    for i in rank_100:\n",
    "        # if station id equals to southern_cross_stop_id, return southern_cross_stop_id, and break the function\n",
    "        if stops_id[int(i)] == southern_cross_stop_id[0]:\n",
    "            return southern_cross_stop_id[0]\n",
    "            break\n",
    "        elif stops_id[int(i)] == southern_cross_stop_id[1]:\n",
    "            return southern_cross_stop_id[1]\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            #for loop in the values in the dictionary of trip_stop \n",
    "            for j in trip_stop.values(): \n",
    "                # if target station id and southern_corss_station id all in trip, return station name and distance\n",
    "                if stops_id[int(i)] in j and southern_cross_stop_id[0] in j and j.index(stops_id[int(i)]) < j.index(southern_cross_stop_id[0]):\n",
    "                    station=stops_id[int(i)]\n",
    "                    distance=round(float(all_distance[int(i)]),3)\n",
    "                    return station\n",
    "                    break\n",
    "                elif stops_id[int(i)] in j and southern_cross_stop_id[1] in j and j.index(stops_id[int(i)]) < j.index(southern_cross_stop_id[1]):\n",
    "                    station=stops_id[int(i)]\n",
    "                    distance=round(float(all_distance[int(i)]),3)\n",
    "                    return station\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the find_station function to find the closest station and the distance between this station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_station_distance(lat,lon):\n",
    "    Find_loc = np.matrix([[lat,lon]])\n",
    "    #run get_distance function to get all distance between station.\n",
    "    all_distance = get_distance(station_lat_lon_mat,Find_loc)\n",
    "    #list 500 lowest distance\n",
    "    rank_10=list(all_distance.argsort(axis=0)[:100])\n",
    "    for i in rank_10:\n",
    "        for j in trip_stop.values(): \n",
    "            # if target station id and southern_corss_station id all in trip, return station name and distance\n",
    "            if stops_id[int(i)] in j and (southern_cross_stop_id[0] in j or southern_cross_stop_id[1] in j):\n",
    "                station=stops_id[int(i)]\n",
    "                distance=round(float(all_distance[int(i)]),3)\n",
    "                return distance\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the find_station_name function to df\n",
    "df['train_station_id']=df.apply(lambda x: find_station_name(x.Lattitude, x.Longtitude),axis=1)\n",
    "# apply the find_station_distance function to df\n",
    "df['distance_to_train_station']=df.apply(lambda x: find_station_distance(x.Lattitude, x.Longtitude),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## travel_min_to_CBD\n",
    "<a id=\"step03\"></a>\n",
    "\n",
    "In this target, we will find the average travel time(minutes) from the closest train station to the Southern Cross Railway Station on weekdays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the average_min function to calcualte the average travel time(minutes) from the closest train station to the Southern Cross Railway Station on weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_min(station_id):\n",
    "    station_id=station_id\n",
    "    if station_id not in southern_cross_stop_id:\n",
    "        trip_list=[]\n",
    "        #for loop in trip_stop dictionary\n",
    "        for key,values in trip_stop.items():\n",
    "            # if station_id and southern_cross_stop_id all in values, key in weekday_trip,and the index of station_id less than southern_cross_stop_id in values, append key to trip_list\n",
    "            if station_id in values and southern_cross_stop_id[0] in values and values.index(station_id)<values.index(southern_cross_stop_id[0]):\n",
    "                if key in weekday_trip:\n",
    "                    trip_list.append(key)\n",
    "            # if station_id and southern_cross_stop_id all in values, key in weekday_trip,and the index of station_id less than southern_cross_stop_id in values, append key to trip_list\n",
    "            elif station_id in values and southern_cross_stop_id[1] in values and values.index(station_id)<values.index(southern_cross_stop_id[1]):\n",
    "                if key in weekday_trip:\n",
    "                    trip_list.append(key)\n",
    "    \n",
    "    \n",
    "        departure_time_list=[]\n",
    "        arrival_time_list=[]\n",
    "    \n",
    "        #for loop in sched_stop_time\n",
    "        for i in sched_stop_time:\n",
    "            stop_id=i.stop_id\n",
    "            trip_id=i.trip_id\n",
    "            arrival_time=i.arrival_time\n",
    "            departure_time=i.departure_time\n",
    "            # if trip_id in trip_list, stop_id equals to station_id, append departure_time to departure_time_list\n",
    "            if trip_id in trip_list and stop_id == station_id: \n",
    "                departure_time_list.append(departure_time)\n",
    "            #if trip_id in trip_list and stop_id equals to southern_cross_stop_id, append arrival_time to arrival_time_list\n",
    "            if trip_id in trip_list and stop_id in southern_cross_stop_id: \n",
    "                arrival_time_list.append(arrival_time)\n",
    "            \n",
    "        #calculate the average by minus arrival_time and departure_time\n",
    "        sum_second=0\n",
    "        count=0\n",
    "        for i in range(len(arrival_time_list)):\n",
    "            sum_second+=(arrival_time_list[i]-departure_time_list[i]).seconds\n",
    "            count+=1\n",
    "\n",
    "        average_min=(sum_second/60)/count\n",
    "    \n",
    "    #if the station_id is equals to southern cross station, the average_min equals to zero\n",
    "    else:\n",
    "        average_min=0\n",
    "    \n",
    "    #return average_min\n",
    "    return average_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['travel_min_to_CBD']=df.apply(lambda x: average_min(x.train_station_id),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## over priced?\n",
    "<a id=\"step04\"></a>\n",
    "In this step, we will find the boolean feature that whether or not the price of the property is higher than the median price of similar properties( with same bedrooms, batheroom, parking_space, and property_type) in the same suburb on the year of selling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = df.apply(lambda x: x.Date[-4:], axis=1) # append a new column which stands for the year of selling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list storing columns used to check similarity\n",
    "similar_list = ['Year',\n",
    "                'Rooms',\n",
    "                'Type',\n",
    "                'Bathroom',\n",
    "                'Car',\n",
    "                'Suburb']\n",
    "\n",
    "# get mean\n",
    "df_mean = df.groupby(similar_list).mean()\n",
    "\n",
    "# get count\n",
    "df_count = df.groupby(similar_list).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define a function to get key for df_mean and df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(x):\n",
    "    key = (x.Year, x.Rooms, x.Type, x.Bathroom, x.Car, x.Suburb)\n",
    "    return key\n",
    "\n",
    "# create a new column named 'over_priced', and if there is no any similar property\n",
    "df['over_priced?'] = df.apply(lambda x: x.Price > df_mean.loc[get_key(x)].Price,\n",
    "                              axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### open councils.txt and create a dictionary which takes suburbs as keys and local government area as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('councils.txt') as councils:\n",
    "    sub_dict = {}\n",
    "    \n",
    "    for line in councils:\n",
    "        # spilt line to a list\n",
    "        line_list = line.strip().split(' :')\n",
    "        \n",
    "        # area means the loval government area\n",
    "        area = line_list[0]\n",
    "        \n",
    "        # transform the suburbs string into a list\n",
    "        sub_list = re.findall(r'\\'(.*?)\\'', line_list[1])\n",
    "        \n",
    "        # add new items into the dictionary\n",
    "        for suburb in sub_list:\n",
    "            sub_dict[suburb.upper()] = area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime A/B/C average\n",
    "<a id=\"step05\"></a>\n",
    "\n",
    "In this step, we calculate the average of type A/B/C crime in the local government area that the property belongs to , in the three years prior to selling the property as the property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open crimebylocationdatatable-yearending31march2016.xlsx, starts from 20th row\n",
    "xl = pd.read_excel('crimebylocationdatatable-yearending31march2016.xlsx',\n",
    "                   sheet_name='Table 1',\n",
    "                   skiprows=19)\n",
    "\n",
    "# drop an useless column\n",
    "crime = xl.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by year, crime type and government area and get summary of each group\n",
    "crime_sum = crime.groupby(['Apr - Mar reference period',\n",
    "                           'CSA Offence Division',\n",
    "                           'Local Government Area']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define functions to get the average of crimes in the three years prior to selling the property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_A(x):\n",
    "    \n",
    "    # check if the key is in the crime_sum\n",
    "    if (int(x.Year)-1, 'A Crimes against the person', sub_dict[x.Suburb]) in crime_sum.index\\\n",
    "    and (int(x.Year)-2, 'A Crimes against the person', sub_dict[x.Suburb]) in crime_sum.index\\\n",
    "    and (int(x.Year)-3, 'A Crimes against the person', sub_dict[x.Suburb]) in crime_sum.index:\n",
    "        \n",
    "        # get the summary of three years\n",
    "        # keys are year, crime type and government area\n",
    "        sum = \\\n",
    "        crime_sum.loc[(int(x.Year)-1, 'A Crimes against the person', sub_dict[x.Suburb])]['Offence Count'] +\\\n",
    "        crime_sum.loc[(int(x.Year)-2, 'A Crimes against the person', sub_dict[x.Suburb])]['Offence Count'] +\\\n",
    "        crime_sum.loc[(int(x.Year)-3, 'A Crimes against the person', sub_dict[x.Suburb])]['Offence Count']\n",
    "        \n",
    "        # get the average\n",
    "        avg = sum/3\n",
    "        \n",
    "    # if the year of selling is earlier than 2015, set it to default value: -1 \n",
    "    else:\n",
    "        avg = -1\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_B(x):\n",
    "    if (int(x.Year)-1, 'B Property and deception offences', sub_dict[x.Suburb]) in crime_sum.index\\\n",
    "    and (int(x.Year)-2, 'B Property and deception offences', sub_dict[x.Suburb]) in crime_sum.index\\\n",
    "    and (int(x.Year)-3, 'B Property and deception offences', sub_dict[x.Suburb]) in crime_sum.index:\n",
    "        sum = \\\n",
    "        crime_sum.loc[(int(x.Year)-1, 'B Property and deception offences', sub_dict[x.Suburb])]['Offence Count'] +\\\n",
    "        crime_sum.loc[(int(x.Year)-2, 'B Property and deception offences', sub_dict[x.Suburb])]['Offence Count'] +\\\n",
    "        crime_sum.loc[(int(x.Year)-3, 'B Property and deception offences', sub_dict[x.Suburb])]['Offence Count']\n",
    "        avg = sum/3\n",
    "    else:\n",
    "        avg = -1\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_C(x):\n",
    "    if (int(x.Year)-1, 'C Drug offences', sub_dict[x.Suburb]) in crime_sum.index\\\n",
    "    and (int(x.Year)-2, 'C Drug offences', sub_dict[x.Suburb]) in crime_sum.index\\\n",
    "    and (int(x.Year)-3, 'C Drug offences', sub_dict[x.Suburb]) in crime_sum.index:\n",
    "        sum = \\\n",
    "        crime_sum.loc[(int(x.Year)-1, 'C Drug offences', sub_dict[x.Suburb])]['Offence Count'] +\\\n",
    "        crime_sum.loc[(int(x.Year)-2, 'C Drug offences', sub_dict[x.Suburb])]['Offence Count'] +\\\n",
    "        crime_sum.loc[(int(x.Year)-3, 'C Drug offences', sub_dict[x.Suburb])]['Offence Count']\n",
    "        avg = sum/3\n",
    "    else:\n",
    "        avg = -1\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply functions for each crime type\n",
    "df['crime_A_average'] = df.apply(lambda x: get_A(x), axis=1)\n",
    "df['crime_B_average'] = df.apply(lambda x: get_B(x), axis=1)\n",
    "df['crime_C_average'] = df.apply(lambda x: get_C(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the closest primary/ Secondary school and the distance (m) between property\n",
    "<a id=\"step06\"></a>\n",
    "\n",
    "In this section, we find the colos the closest primary school to the property. In addition, we will calculate the direct distance between the property and the closest primary school.\n",
    "\n",
    "We will use bsoup to parse the xml file and extract the data we need, and then use get_distance function to calculate the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use bsoup to open the schools.xml file\n",
    "soup = bsoup(open('schools.xml'),\"lxml-xml\")\n",
    "school_name_list=[]\n",
    "p_school_list=[]\n",
    "p_school_lat_lon=[]\n",
    "\n",
    "s_school_list=[]\n",
    "s_school_lat_lon=[]\n",
    "\n",
    "# for loop in soup.find_all('school')\n",
    "for i in soup.find_all('school'):\n",
    "        School_name=i.find(\"School_Name\").string \n",
    "        Education_Sector=i.find(\"Education_Sector\").string \n",
    "        Entity_Type=i.find(\"Entity_Type\").string \n",
    "        School_No=i.find(\"School_No\").string \n",
    "        School_Type=i.find(\"School_Type\").string \n",
    "        Full_Phone_No=i.find(\"Full_Phone_No\").string \n",
    "        LGA_ID=i.find(\"LGA_ID\").string \n",
    "        LGA_Name=i.find(\"LGA_Name\").string \n",
    "        X=float(i.find(\"X\").string)\n",
    "        Y=float(i.find(\"Y\").string)\n",
    "        school_name_list.append(School_name)\n",
    "        #if school_type equals to 'Primary' or 'Pri/Sec', append school_name to p_school_list and append lat_lon to p_school_lat_lon\n",
    "        if School_Type == 'Primary' or School_Type == 'Pri/Sec':\n",
    "            p_school_list.append(School_name)\n",
    "            lat_lon=[Y,X]\n",
    "            p_school_lat_lon.append(lat_lon)\n",
    "        #if school_type equals to 'Secondary' or 'Pri/Sec', append school_name to s_school_list and append lat_lon to s_school_lat_lon\n",
    "        if School_Type == 'Secondary' or School_Type == 'Pri/Sec':\n",
    "            s_school_list.append(School_name)\n",
    "            lat_lon=[Y,X]\n",
    "            s_school_lat_lon.append(lat_lon)\n",
    "            \n",
    "# translate the list of p_school_lat_lon_matrix to matrix            \n",
    "p_school_lat_lon_matrix=np.matrix(p_school_lat_lon)  \n",
    "# translate the list of s_school_lat_lon_matrix to matrix    \n",
    "s_school_lat_lon_matrix=np.matrix(s_school_lat_lon)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the primary_school)name function to find the closest primary school."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primary_school_name(lat,lon):\n",
    "    #matrix the property lat and lon\n",
    "    Find_loc = np.matrix([[lat,lon]])\n",
    "    #run get_distance function to get all distance between propery and primary school.\n",
    "    all_distance = get_distance(p_school_lat_lon_matrix,Find_loc)\n",
    "    school_name = p_school_list[int(all_distance.argmin(axis=0))]\n",
    "    distance = round(float(all_distance[int(all_distance.argmin(axis=0))]),3)\n",
    "    \n",
    "    return school_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the primary_school_distance function to find the closest primary school and the distance between it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primary_school_distance(lat,lon):\n",
    "    #matrix the property lat and lon\n",
    "    Find_loc = np.matrix([[lat,lon]])\n",
    "    #run get_distance function to get all distance between propery and primary school.\n",
    "    all_distance = get_distance(p_school_lat_lon_matrix,Find_loc)\n",
    "    school_name = p_school_list[int(all_distance.argmin(axis=0))]\n",
    "    distance = round(float(all_distance[int(all_distance.argmin(axis=0))]),3)\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the secondary_school)name function to find the closest secondary school and the distance between it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secondary_school_name(lat,lon):\n",
    "    Find_loc = np.matrix([[lat,lon]])\n",
    "    #run get_distance function to get all distance between property and secondary school.\n",
    "    all_distance = get_distance(s_school_lat_lon_matrix,Find_loc)\n",
    "    school_name = s_school_list[int(all_distance.argmin(axis=0))]\n",
    "    distance = round(float(all_distance[int(all_distance.argmin(axis=0))]),3)\n",
    "    \n",
    "    return school_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the secondary_school_distance function to find the closest secondary school and the distance between it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secondary_school_distance(lat,lon):\n",
    "    Find_loc = np.matrix([[lat,lon]])\n",
    "    #run get_distance function to get all distance between property and secondary school.\n",
    "    all_distance = get_distance(s_school_lat_lon_matrix,Find_loc)\n",
    "    school_name = s_school_list[int(all_distance.argmin(axis=0))]\n",
    "    distance = round(float(all_distance[int(all_distance.argmin(axis=0))]),3)\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply primary_school_name function to df\n",
    "df['closest_primary_school']=df.apply(lambda x: primary_school_name(x.Lattitude, x.Longtitude),axis=1)\n",
    "#apply primary_school_distance function to df\n",
    "df['distance_to_closest_primary']=df.apply(lambda x: primary_school_distance(x.Lattitude, x.Longtitude),axis=1)\n",
    "#apply secondary_school_name function to df\n",
    "df['closest_secondary_school']=df.apply(lambda x: secondary_school_name(x.Lattitude, x.Longtitude),axis=1)\n",
    "#apply secondary_school_distance function to df\n",
    "df['distance_to_closest secondary']=df.apply(lambda x: secondary_school_distance(x.Lattitude, x.Longtitude),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary / Secondary School Ranking\n",
    "<a id=\"step07\"></a>\n",
    "\n",
    "In this step, we input the rank of the closest primary/ secondary to the propery.\n",
    "\n",
    "The rank we are using is from web, therefore, we need to use Web Scraping to get the rank which we want.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the urlopen() function to open the web\n",
    "html = urlopen(\"http://www.schoolcatchment.com.au/?p=12301\")\n",
    "bsobj = bsoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_school_rank={}\n",
    "\n",
    "# for loop in bsobj.find_all('table')\n",
    "for i in bsobj.find_all('table'):\n",
    "    School_rank=i.find_all(\"td\",{'class':'column-1'})\n",
    "    School_name=i.find_all(\"td\",{'class':'column-2'})    \n",
    "    # for loop in zip(School_name, School_rank)\n",
    "    for name, rank in zip(School_name, School_rank):\n",
    "        #if name not in the dictionary of primary_school_rank, set the value to rank\n",
    "        if name.string not in primary_school_rank.keys():\n",
    "            primary_school_rank[name.string]= int(rank.string)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this step, we will to check if there are some mismatch between school rank and p_school_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_primary=[]\n",
    "for i in primary_school_rank.keys():\n",
    "    if i not in p_school_list:\n",
    "        wrong_primary.append(i)\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we could notice that there are some mistake in school rank, therefor, we use get_close_matches function to fix the problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_primary_dict={}\n",
    "\n",
    "#for loop in wrong_primary\n",
    "for i in wrong_primary:\n",
    "    school=difflib.get_close_matches(i, p_school_list, n=1, cutoff=0.90)\n",
    "    # if the length of school better than zero, set the wrong school name as key and correct school name as value \n",
    "    if len(school) >0:\n",
    "        for j in school:\n",
    "            correct_primary_dict[i]=j\n",
    "\n",
    "#for loop to change the wrong school name to correct school name         \n",
    "for i in correct_primary_dict.keys():\n",
    "    if i in primary_school_rank.keys():\n",
    "        primary_school_rank[correct_primary_dict[i]]=primary_school_rank[i]\n",
    "        del primary_school_rank[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the primary_rank function to input the primary school rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primary_rank(primary_school):\n",
    "    primary_school_name = primary_school\n",
    "    # if primary_school_name in the keys in primary_school_rank,set the value in the dictionary of primary_school_rank as rank \n",
    "    if primary_school_name in primary_school_rank.keys():\n",
    "        rank = primary_school_rank[primary_school_name]\n",
    "    else:\n",
    "        rank = 'not ranked'\n",
    "    \n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply primary_rank function to df\n",
    "df['primary_school_ranking']=df.apply(lambda x: primary_rank(x.closest_primary_school),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_school=[]\n",
    "text=[]\n",
    "# input the file\n",
    "temp = open('Temp.txt', 'w')\n",
    "\n",
    "# set the identify patterns\n",
    "pattern_name=re.compile('^<div>([a-zA-Z].*?)<')\n",
    "\n",
    "#open the 'secondary-school-ranking.htm' file\n",
    "with open('secondary-school-ranking.htm') as fp:\n",
    "    # read each line in file\n",
    "    for line in fp:\n",
    "        #if line is not end with '</div><hr />\\n', '<hr />\\n', or '<hr /></ul>\\n' replace \\n to ' '\n",
    "        if line != '</div><hr />\\n' or line !='<hr />\\n' or line !='<hr /></ul>\\n':\n",
    "            line = line.replace('\\n',' ')\n",
    "            line = line.replace('N/A','')\n",
    "            temp.write(line)\n",
    "        else:\n",
    "            temp.write(line)                \n",
    "        \n",
    "        #strip the line\n",
    "        lines=line.strip()\n",
    "        #append lines to text list\n",
    "        text.append(lines)\n",
    "\n",
    "# read each line in text        \n",
    "for line in text:\n",
    "    search_name = pattern_name.search(line)\n",
    "    #if search_name is not None append name to secondary_school\n",
    "    if search_name is not None:\n",
    "        name=search_name.group(1)\n",
    "        if name != 'N/A':\n",
    "            secondary_school.append(name)\n",
    "\n",
    "#close the file\n",
    "temp.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the file\n",
    "temp = open('Temp.txt', 'r')\n",
    "rank_list=[]\n",
    "\n",
    "# set the identify patterns\n",
    "pattern_rank=re.compile('<div>[a-zA-Z].*?</div> <div>(\\d.*?)<')\n",
    "\n",
    "# read each line in temp\n",
    "for line in temp:    \n",
    "    find_rank = pattern_rank.findall(line)\n",
    "    # for loop in find_rank, and append rank to rank_list\n",
    "    for rank in find_rank:\n",
    "        rank_list.append(rank)\n",
    "\n",
    "#close the file       \n",
    "temp.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Secondary_school_rank={}\n",
    "#for loop in zip(secondary_school,rank_list)\n",
    "for school,rank in zip(secondary_school,rank_list):\n",
    "    # if school not in secondary_school_rank, set school as key, rank as value in dictionary of Secondary_school_rank\n",
    "    if school not in Secondary_school_rank.keys():\n",
    "        Secondary_school_rank[school]=rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this step, we will to check if there are some mismatch between school rank and s_school_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_Secondary=[]\n",
    "for i in Secondary_school_rank.keys():\n",
    "    if i not in s_school_list:\n",
    "        wrong_Secondary.append(i)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we could notice that there are some mistake in school rank, therefor, we use get_close_matches function to fix the problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_Secondary_dict={}\n",
    "\n",
    "#for loop in wrong_secondary\n",
    "for i in wrong_Secondary:\n",
    "    school=difflib.get_close_matches(i, s_school_list, n=1, cutoff=0.85)\n",
    "    # if the length of school better than zero, set the wrong school name as key and correct school name as value \n",
    "    if len(school) >0:\n",
    "        for j in school:\n",
    "            correct_Secondary_dict[i]=j\n",
    "            \n",
    "#for loop to change the wrong school name to correct school name\n",
    "for i in correct_Secondary_dict.keys():\n",
    "    if i in Secondary_school_rank.keys():\n",
    "        Secondary_school_rank[correct_Secondary_dict[i]]=Secondary_school_rank[i]\n",
    "        del Secondary_school_rank[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the secondary_rank function to input the secondary school rank\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secondary_rank(secondary_school):\n",
    "    secondary_school_name = secondary_school\n",
    "    # if secondary_school_name in the keys in secondary_school_rank,set the value in the dictionary of Secondary_school_rank as rank \n",
    "    if secondary_school_name in Secondary_school_rank.keys():\n",
    "        rank = int(Secondary_school_rank[secondary_school_name])\n",
    "    else:\n",
    "        rank = 'not ranked'\n",
    "    \n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply secondary_rank function to df\n",
    "df['secondary_school_ranking']=df.apply(lambda x: secondary_rank(x.closest_secondary_school),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['ID','Address','Suburb','Price','Type','Date','Rooms','Bathroom','Car','Landsize','Age','Lattitude','Longtitude','train_station_id','distance_to_train_station','travel_min_to_CBD','over_priced?','crime_A_average','crime_B_average','crime_C_average','closest_primary_school','distance_to_closest_primary','primary_school_ranking','closest_secondary_school','distance_to_closest secondary','secondary_school_ranking']]\n",
    "df=df.set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Group081_solution.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Data Reshaping\n",
    "<a id=\"task_2\"></a>\n",
    "In this task, we use different normalization and transformation methods, such as standardization, min-max normalization, log, power, and root transformation on Rooms, crime_C_acerage, travel_min_to_CBD, and property_age to find which normalization/transformation method would work better to fit the linear model on price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Group081_solution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns used in the linear regression and save in temp\n",
    "col_list = ['Price', 'Rooms', 'crime_C_average', 'Age', 'travel_min_to_CBD']\n",
    "temp = df[col_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define functions to process normalisations and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_normalisation(df, col):\n",
    "    std_scale = preprocessing.StandardScaler().fit(df[[col]])\n",
    "    df_std = std_scale.transform(df[[col]])\n",
    "    name = 'std_' + col\n",
    "    df[name] = df_std[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_normalisation(df, col):\n",
    "    minmax_scale = preprocessing.MinMaxScaler().fit(df[[col]])\n",
    "    name = 'minmax_' + col\n",
    "    df[name] = minmax_scale.transform(df[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_transformation(df, col):\n",
    "    name = 'root_' + col\n",
    "    df[name] = None\n",
    "    i = 0\n",
    "    for row in df.iterrows():\n",
    "        df[name].at[i] = math.sqrt(df[col][i])\n",
    "        i += 1\n",
    "    df[name] = df[name].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_transformation(df, col):\n",
    "    name = 'square_' + col\n",
    "    df[name] = None\n",
    "    i = 0\n",
    "    for row in df.iterrows():\n",
    "        df[name].at[i] = math.pow(df[col][i],2)\n",
    "        i += 1\n",
    "    df[name] = df[name].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transformation(df, col):\n",
    "    name = 'log_' + col\n",
    "    df[name] = None\n",
    "    i = 0\n",
    "    for row in df.iterrows():\n",
    "        df[name].at[i] = math.log(df[col][i])\n",
    "        i += 1\n",
    "\n",
    "    df[name] = df[name].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the statistical characters of the temp\n",
    "temp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of **Price** are way larger than other values in other columns. In this situation, I suppose that **log transformation** is more suitable. And looking at the temp dataframe, it is clear that the values of log_Price column's standard error is less than others. As for the **Age** column, since the difference between min and max is large, as well as the standard error is large, using **log transformation** can solve this problem. Also due to this reason, I think log transformation is suitable for the **crime_C_average** column. As for the **travel_min_to_CBD column**, it contains the value of 0 which means it can not be applied log transformation, in this case, I assume it is used **root transformation**. When it comes to the **Rooms** column, its values are basically in a small range, so I do not think it requires any normalisation or transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply log transformation in Price, Age, crime_C_average\n",
    "for col in ['Price', 'Age', 'crime_C_average']:\n",
    "    log_transformation(temp, col)\n",
    "    \n",
    "# apply root transformation in travel_min_to_CBD\n",
    "root_transformation(temp,'travel_min_to_CBD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each variables in one graph\n",
    "temp.log_Age.plot(color = 'red')\n",
    "temp.log_crime_C_average.plot(color = 'blue')\n",
    "temp.root_travel_min_to_CBD.plot(color = 'green')\n",
    "temp.Rooms.plot(color = 'yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each column, plot the col - price scatter graph.\n",
    "for col in ['log_Age','log_crime_C_average','root_travel_min_to_CBD','Rooms']:\n",
    "    plt.scatter(temp[col], temp.log_Price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the two graphs above, I think **applying log transformation in crime_C_average, Age, and Price**, **applying root transformation in travel_min_to_CBD** is a good choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build up the model by parameters above using ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.ols(formula=\"log_Price ~ log_Age \\\n",
    "+ log_crime_C_average\\\n",
    "+ root_travel_min_to_CBD\\\n",
    "+ Rooms\"\\\n",
    "               ,data=temp).fit() # fit the model\n",
    "print(model.summary()) # show details of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot predicted values and real values(log_Price) in one graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fitted = model.fittedvalues\n",
    "y_fitted.plot(color='red')\n",
    "temp.log_Price.plot(color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "<a id=\"summary\"></a>\n",
    "\n",
    "In this assignment, we learn how to integrate serveral dataets into one signle schema and fond and fix possible problems in the data by using Python 3. In addition, we use different normalization and transformation methods, such as standardization, min-max normalization, log, power, and root transformation on Rooms, crime_C_acerage, travel_min_to_CBD, and property_age to find which normalization/transformation method would work better to fit the linear model on price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
